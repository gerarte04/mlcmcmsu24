{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad1c31c-be4d-4295-a9bf-ed4258041c3d",
   "metadata": {},
   "source": [
    "## Основные понятия и задачи машинного обучения\n",
    "\n",
    "**Машинное обучение (machine learning, ML)** – наука, позволяющая компьютерам принимать сложные решения без явной спецификации алгоритма принятия решения. Компьютер учится на заданном опыте решать некоторый класс задач относительно некоторого показателя качества, если этот показатель растет на классе задач после получения опыта.\n",
    "\n",
    "### Достоинства машинного обучения\n",
    "* точность (люди могут извлекать лишь простые правила и хуже оперируют большими объемами признаков, а машинное обучение способно улавливать тонкие закономерности)\n",
    "* быстрота и низкая стоимость разработки (сокращается количество специалистов)\n",
    "### Нюансы\n",
    "* для обучения модели нужен некоторый \"опыт\" (обучающая выборка)\n",
    "* для настройки моделей требуется больше вычислительных ресурсов\n",
    "\n",
    "### Примеры задач машинного обучения\n",
    "* предсказать, уйдёт ли клиент к конкурентам (churn prediction)\n",
    "* является ли последовательность финансовых транзакций мошеннической (fraud detection)\n",
    "* предсказание пробок и времени в пути при планировании маршрута (traffic prediction)\n",
    "* стоит ли показывать заданный товар покупателю в качестве рекомендации (recommender systems)\n",
    "* рекомендовать ли человека в качестве друга в социальной сети\n",
    "* голосовой ассистент: распознавание речи, автоматический ответ на вопросы, генерация речевого ответа\n",
    "* идентификация человека по лицу\n",
    "* распознавание номера машины на камерах\n",
    "* и т. д. и т. п.\n",
    "\n",
    "### Виды обучения\n",
    "* **Машинное обучение** – в общем и целом настройка прогнозирующих алгоритмов  \n",
    "* **Глубокое обучение** – сложные многоуровневые модели (нейросети), оптимальный вектор признаков генерируется автоматически  \n",
    "* **Обучение с подкреплением** – выработка интерактивной стратегии поведения в изменяемой среде, мышление вперед (например, шахматы, игровые персонажи, автомобили, дроны, роботы)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c09e0c-eb66-4911-9b67-2334fcc8e164",
   "metadata": {},
   "source": [
    "## Формальное определение задачи\n",
    "Каждый объект из некоторого класса $Z$ описывается вектором известных характеристик **(признаков)** $x \\in X$ и вектором предсказываемых характеристик **(откликов)** $y \\in Y$. $$z = (x, y) \\in Z$$\n",
    "Требуется найти такое отображение $f: X \\rightarrow Y$, которое точно описывало бы взаимосвязь этих множеств для любых возможных значений $x$, используя конечный набор известных пар $(x, y)$ **(обучающую выборку)**.  \n",
    "\n",
    "Входное описание объекта $x \\in X$ состоит из индивидуальных признаков $x^i \\in X_i$. Они могут быть следующих типов:\n",
    "* $X_i = \\mathbb{R}$  –  **вещественные (количественные)**: денежная стоимость, расстояние и т. п.\n",
    "* $X_i = \\{0, 1\\}$  –  **бинарные**: да/нет\n",
    "* $|X_i| < \\infty$  –  **дискретные категориальные**: профессия, тип товара, ...\n",
    "* $|X_i| < \\infty$, элементы $x \\in X_i$ упорядочены   –  **дискретные упорядоченные**: место в гонке, уровень образования\n",
    "\n",
    "### Возможные постановки задачи\n",
    "* **обучение с учителем (supervised learning)** – дана обучающая выборка $\\{(x_i, y_i): i = 1, 2, ..., n\\}$, модель обучается принудительно по известным откликам\n",
    "    * иногда возможно **трансдуктивное обучение** – когда все возможные входы $x$ известны заранее\n",
    "* **обучение без учителя (unsupervised learning)** – даны только входы $x_1, x_2, ..., x_n$ и некоторые метрические признаки взаимосвязи (\"расстояния\") между объектами, модель обучается спонтанно, самостоятельно обнаруживая закономерности\n",
    "\n",
    "Примеры задач на обучение с учителем:\n",
    "* **регрессия** – прогнозирование $y \\in \\mathbb{R}$ для любых значений $x$, построение функциональной зависимости $y$ от $x$.\n",
    "* **классификация** – прогнозирование дискретного $y$, назначение объекта одному из заданных классов\n",
    "* **ранжирование**\n",
    "\n",
    "Примеры задач на обучение без учителя:\n",
    "* **кластеризация** – разбиение объектов на похожие группы\n",
    "* **снижение размерности** – переход к низшей размерности с минимальным искажением геометрии\n",
    "* **обнаружение аномалий** – выделение нетипичных, нестандартно распределенных объектов\n",
    "* **поиск ассоциативных правил** – по наборам множеств $\\{a, b, c\\}, \\{a, d, e\\}, \\{a, b\\}, \\{a, b, g , h\\}$ генерировать правила: $a → b, b → a, ...$ (например, анализируя потребительские корзины, можно установить, что масло хорошо сочетается с хлебом)\n",
    "\n",
    "Простой пример - регрессия (предсказать ожидаемый y для любого x), аппроксимируя регрессионной кривой. Классификация - по набору признаков отнести объект к какой-либо категории.\n",
    "\n",
    "Обучение с учителем - дана обучающая выборка {(xi, yi)} (м.б. трансдуктивное обучение - входные x известны заранее).  \n",
    "Без учителя - даны лишь некоторые метрические признаки взаимосвязи между объектами (например, кластеризация, снижение размерности (переход к низшей размерности с минимальным искажение геометрии), обнаружение аномалий (выделение нетипичных, нестандартно распределенных объектов), поиск ассоциативных правил (масло покупается с хлебом), ранжирование).\n",
    "\n",
    "Регрессия представляется параметрозованной функцией y = g(x). Параметризация прогноза дискретной сущности: для каждого класса настраивается такая функция, выбираем тот класс, функция которого принимает максимальное значение\n",
    "\n",
    "Точность предсказаний может оцениваться критерием качества (score function) и функцией потерь (loss function): i.e. y^-y\n",
    "\n",
    "Обучающая выборка задается матрицей признаков X и вектором откликов y. Тестовая выборка может быть задана аналогично: X', нужно найти y'.\n",
    "\n",
    "Кривая обучения задает типичную зависимость от N для параметрических моделей. Чем больше N, тем потери на обучении ближе к потерям на тесте.\n",
    "\n",
    "Проблема недообучения: модель слишком простая для реальных данных (не улавливает тонких закономерностей)\n",
    "Проблема переобучения: слишком сложная (настраивается на шум в измерениях), бывает, когда слишком много параметров (близко к размеру обучающей выборки)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e97b345-d8a9-416f-8376-389464202452",
   "metadata": {},
   "source": [
    "## Функциональные классы\n",
    "\n",
    "### Пример линейного класса функций\n",
    "* регрессия: $Y = \\mathbb{R};\\quad \\hat{y} = g(x),\\: g(x)$ параметризовано $\\theta$.\n",
    "* Многоклассовый классификатор ($Y = \\{1, 2, \\ldots, C\\}$):  \n",
    "  $\\hat{y}(x) = \\arg\\max\\limits_{c}g_c(x),\\: g(x)$ параметризовано $\\theta$  \n",
    "  $\\{x: g_i(x) = g_j(x)\\}$ – граница между классами $i, j$  \n",
    "  $M(x, y) = g_y(x) − \\max\\limits_{c \\ne y}g_c(x)$ – отступ (качество классификации)\n",
    "* Бинарный классификатор ($y \\in \\{+1, −1\\}$):  \n",
    "  $\\DeclareMathOperator{\\sign}{sign} \\hat{y}(x) = \\arg\\max\\limits_{c \\in \\{+1,−1\\}}g_c(x) = \\sign (g_{+1}(x) − g_{−1}(x)) = \\sign g(x)$  \n",
    "  $M(x, y) = g_y(x) − g_{−y}(x) = y(g_{+1}(x) − g_{−1}(x)) = yg(x)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f19e8-d36c-4977-811c-1227ece466a6",
   "metadata": {},
   "source": [
    "## Обучающие материалы\n",
    "\n",
    "* Документация sklearn\n",
    "* [kaggle.com](kaggle.com) (соревнования по МЛ)\n",
    "* [paperswithcode.com](paperswithcode.com) (описание/код методов, только нейросети)\n",
    "\n",
    "## Датасеты\n",
    "* Репозиторий UCI (небольшие датасеты)\n",
    "* [openml.org](openml.org) (много датасетов, есть большие)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5427afc-804d-4558-85b9-b57f232b0d36",
   "metadata": {},
   "source": [
    "## Метрические методы\n",
    "\n",
    "3-классовый метод ближайших центроидов\n",
    "метод k ближайших соседей (классификация - найти k ближайших объектов в обучающей выборке к заданному x и сопоставить x ч самый частотный класс среди k ближайших объектов, регресия - сопоставить x среднему отклику). У него можно варьировать кол-во соседей, функцию расстояния Его легко реализовать, он простой, интерпретируемый (легко понимать, почему следует принимать решение, т. к. есть похожие известные случаи), но велика сложность O(ND) и плохо влияет повышение размерности (близких точек становится мало). Масштабирование признаков влияет на прогнозы KNN, поэтому желательно их нормализировать (стандартизация, нормализация средним или применить диапазонное шкалирование)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514b0acd",
   "metadata": {},
   "source": [
    "заебало писать короче"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
